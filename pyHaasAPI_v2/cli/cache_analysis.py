#!/usr/bin/env python3
"""
Advanced Cache Analysis CLI for pyHaasAPI v2

This tool provides sophisticated cache analysis functionality based on the excellent
v1 implementation from analyze_from_cache.py, including manual data extraction,
advanced filtering, and comprehensive reporting.

Features:
- Manual data extraction from cached files
- Advanced filtering with realistic criteria
- Data distribution analysis
- Comprehensive reporting (JSON, CSV, Markdown)
- Risk and stability scoring
- Interactive analysis capabilities
"""

import asyncio
import argparse
import json
import csv
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path

from ..core.logging import get_logger
from ..services.analysis.analysis_service import AnalysisService, BacktestPerformance, LabAnalysisResult
from ..core.client import AsyncHaasClient
from ..core.auth import AuthenticationManager
from ..api.lab import LabAPI
from ..api.backtest import BacktestAPI
from ..api.bot import BotAPI

logger = get_logger("cache_analysis")


class CacheAnalyzer:
    """Advanced cache analysis tool based on excellent v1 implementation"""
    
    def __init__(self):
        self.client = None
        self.auth_manager = None
        self.analysis_service = None
        self.logger = get_logger("cache_analyzer")
        
    async def initialize(self) -> bool:
        """Initialize the analyzer with API connections"""
        try:
            # Initialize client and auth
            self.client = AsyncHaasClient()
            self.auth_manager = AuthenticationManager(self.client)
            
            # Initialize API modules
            lab_api = LabAPI(self.client, self.auth_manager)
            backtest_api = BacktestAPI(self.client, self.auth_manager)
            bot_api = BotAPI(self.client, self.auth_manager)
            
            # Initialize analysis service
            self.analysis_service = AnalysisService(
                lab_api, backtest_api, bot_api, self.client, self.auth_manager
            )
            
            # Authenticate
            await self.auth_manager.authenticate()
            
            self.logger.info("‚úÖ Cache analyzer initialized successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Failed to initialize cache analyzer: {e}")
            return False
    
    async def analyze_cached_labs(
        self, 
        lab_ids: Optional[List[str]] = None,
        top_count: int = 10,
        criteria: Optional[Dict[str, Any]] = None
    ) -> List[LabAnalysisResult]:
        """
        Analyze cached labs with advanced filtering
        
        Args:
            lab_ids: Optional list of specific lab IDs to analyze
            top_count: Number of top performers to return per lab
            criteria: Optional filtering criteria
            
        Returns:
            List of LabAnalysisResult objects
        """
        try:
            self.logger.info("üîç Starting cached lab analysis...")
            
            # Get all labs if no specific IDs provided
            if not lab_ids:
                labs = await self.analysis_service.lab_api.get_labs()
                lab_ids = [lab.lab_id for lab in labs]
            
            self.logger.info(f"üìä Analyzing {len(lab_ids)} labs")
            
            results = []
            
            for lab_id in lab_ids:
                try:
                    # Use manual analysis for proper data extraction
                    performances = await self.analysis_service.analyze_lab_manual(lab_id, top_count)
                    
                    if performances:
                        # Get lab details
                        lab_details = await self.analysis_service.lab_api.get_lab_details(lab_id)
                        
                        # Calculate summary statistics
                        rois = [p.roi_percentage for p in performances]
                        win_rates = [p.win_rate for p in performances]
                        
                        result = LabAnalysisResult(
                            lab_id=lab_id,
                            lab_name=lab_details.lab_name if lab_details else f"Lab {lab_id[:8]}",
                            script_name=performances[0].script_name if performances else "Unknown",
                            market_tag=performances[0].market_tag if performances else "Unknown",
                            total_backtests=len(performances),
                            top_performers=performances,
                            average_roi=sum(rois) / len(rois) if rois else 0.0,
                            best_roi=max(rois) if rois else 0.0,
                            average_win_rate=sum(win_rates) / len(win_rates) if win_rates else 0.0,
                            best_win_rate=max(win_rates) if win_rates else 0.0,
                            analysis_timestamp=datetime.now().isoformat(),
                            success=True
                        )
                        results.append(result)
                        
                        self.logger.info(f"‚úÖ Analyzed {lab_id[:8]} - {len(performances)} backtests")
                    else:
                        self.logger.warning(f"‚ö†Ô∏è No backtests found for {lab_id[:8]}")
                        
                except Exception as e:
                    self.logger.error(f"‚ùå Error analyzing {lab_id[:8]}: {e}")
                    continue
            
            self.logger.info(f"üéØ Analysis complete - {len(results)} labs analyzed")
            return results
            
        except Exception as e:
            self.logger.error(f"‚ùå Cache analysis failed: {e}")
            return []
    
    async def show_data_distribution(self, lab_results: List[LabAnalysisResult]) -> None:
        """Show data distribution analysis"""
        try:
            distribution = await self.analysis_service.analyze_data_distribution(lab_results)
            
            if "error" in distribution:
                self.logger.error(f"‚ùå Distribution analysis failed: {distribution['error']}")
                return
            
            print("\n" + "="*60)
            print("üìä DATA DISTRIBUTION ANALYSIS")
            print("="*60)
            
            print(f"Total Backtests: {distribution['total_backtests']}")
            
            # ROI Distribution
            roi_dist = distribution['roi_distribution']
            print(f"\nROI Distribution:")
            print(f"  Min: {roi_dist['min']:.2f}%")
            print(f"  Max: {roi_dist['max']:.2f}%")
            print(f"  Avg: {roi_dist['avg']:.2f}%")
            print(f"  Median: {roi_dist['median']:.2f}%")
            
            # Win Rate Distribution
            wr_dist = distribution['win_rate_distribution']
            print(f"\nWin Rate Distribution:")
            print(f"  Min: {wr_dist['min']:.2f}%")
            print(f"  Max: {wr_dist['max']:.2f}%")
            print(f"  Avg: {wr_dist['avg']:.2f}%")
            print(f"  Median: {wr_dist['median']:.2f}%")
            
            # Trades Distribution
            trades_dist = distribution['trades_distribution']
            print(f"\nTrades Distribution:")
            print(f"  Min: {trades_dist['min']}")
            print(f"  Max: {trades_dist['max']}")
            print(f"  Avg: {trades_dist['avg']:.1f}")
            print(f"  Median: {trades_dist['median']}")
            
            # Drawdown Distribution
            dd_dist = distribution['drawdown_distribution']
            print(f"\nDrawdown Distribution:")
            print(f"  Min: {dd_dist['min']:.2f}%")
            print(f"  Max: {dd_dist['max']:.2f}%")
            print(f"  Avg: {dd_dist['avg']:.2f}%")
            print(f"  Median: {dd_dist['median']:.2f}%")
            
            print("="*60)
            
        except Exception as e:
            self.logger.error(f"‚ùå Error showing data distribution: {e}")
    
    async def generate_reports(
        self, 
        lab_results: List[LabAnalysisResult],
        criteria: Optional[Dict[str, Any]] = None,
        save_results: bool = False
    ) -> Dict[str, Any]:
        """Generate comprehensive analysis reports"""
        try:
            self.logger.info("üìã Generating analysis reports...")
            
            # Generate filtered reports
            reports = await self.analysis_service.generate_lab_analysis_reports(lab_results, criteria)
            
            if not reports:
                self.logger.warning("‚ö†Ô∏è No qualifying backtests found with current criteria")
                return {}
            
            # Display reports
            print("\n" + "="*80)
            print("üìã LAB ANALYSIS REPORTS")
            print("="*80)
            
            for lab_id, report in reports.items():
                print(f"\nüî¨ Lab: {report['lab_name']} ({lab_id[:8]})")
                print(f"   Script: {report['script_name']}")
                print(f"   Market: {report['market_tag']}")
                print(f"   Total Backtests: {report['total_backtests']}")
                print(f"   Qualifying Bots: {report['qualifying_bots']}")
                print(f"   Average ROI: {report['average_roi']:.2f}%")
                print(f"   Best ROI: {report['best_roi']:.2f}%")
                print(f"   Average Win Rate: {report['average_win_rate']:.2f}%")
                print(f"   Best Win Rate: {report['best_win_rate']:.2f}%")
                
                # Show top performers
                if report['top_performers']:
                    print(f"   Top Performers:")
                    for i, perf in enumerate(report['top_performers'][:5], 1):
                        roe = (perf.realized_profits_usdt / max(perf.starting_balance, 1)) * 100
                        print(f"     {i}. ROI: {perf.roi_percentage:.2f}% | ROE: {roe:.2f}% | WR: {perf.win_rate*100:.1f}% | Trades: {perf.total_trades}")
            
            print("="*80)
            
            # Save results if requested
            if save_results:
                await self.save_reports_to_file(reports)
            
            return reports
            
        except Exception as e:
            self.logger.error(f"‚ùå Error generating reports: {e}")
            return {}
    
    async def save_reports_to_file(self, reports: Dict[str, Any]) -> None:
        """Save reports to JSON and CSV files"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # Save JSON report
            json_file = f"cache_analysis_report_{timestamp}.json"
            with open(json_file, 'w') as f:
                json.dump(reports, f, indent=2, default=str)
            
            # Save CSV report
            csv_file = f"cache_analysis_report_{timestamp}.csv"
            with open(csv_file, 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow([
                    'Lab ID', 'Lab Name', 'Script Name', 'Market Tag',
                    'Total Backtests', 'Qualifying Bots', 'Average ROI', 'Best ROI',
                    'Average Win Rate', 'Best Win Rate'
                ])
                
                for lab_id, report in reports.items():
                    writer.writerow([
                        lab_id[:8],
                        report['lab_name'],
                        report['script_name'],
                        report['market_tag'],
                        report['total_backtests'],
                        report['qualifying_bots'],
                        f"{report['average_roi']:.2f}%",
                        f"{report['best_roi']:.2f}%",
                        f"{report['average_win_rate']:.2f}%",
                        f"{report['best_win_rate']:.2f}%"
                    ])
            
            self.logger.info(f"üíæ Reports saved to {json_file} and {csv_file}")
            
        except Exception as e:
            self.logger.error(f"‚ùå Error saving reports: {e}")


async def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description="Advanced Cache Analysis CLI for pyHaasAPI v2")
    
    # Analysis options
    parser.add_argument('--lab-ids', help='Comma-separated list of lab IDs to analyze')
    parser.add_argument('--top-count', type=int, default=10, help='Number of top performers per lab')
    parser.add_argument('--show-data-distribution', action='store_true', help='Show data distribution analysis')
    parser.add_argument('--generate-reports', action='store_true', help='Generate and display analysis reports')
    parser.add_argument('--save-results', action='store_true', help='Save results to files')
    
    # Filtering criteria
    parser.add_argument('--min-roe', type=float, help='Minimum ROE percentage')
    parser.add_argument('--max-roe', type=float, help='Maximum ROE percentage')
    parser.add_argument('--min-winrate', type=float, default=30, help='Minimum win rate percentage')
    parser.add_argument('--max-winrate', type=float, help='Maximum win rate percentage')
    parser.add_argument('--min-trades', type=int, default=5, help='Minimum number of trades')
    parser.add_argument('--max-trades', type=int, help='Maximum number of trades')
    
    args = parser.parse_args()
    
    # Initialize analyzer
    analyzer = CacheAnalyzer()
    if not await analyzer.initialize():
        return 1
    
    try:
        # Parse lab IDs
        lab_ids = None
        if args.lab_ids:
            lab_ids = [lab_id.strip() for lab_id in args.lab_ids.split(',')]
        
        # Set up filtering criteria
        criteria = {
            'min_roe': args.min_roe,
            'max_roe': args.max_roe,
            'min_winrate': args.min_winrate,
            'max_winrate': args.max_winrate,
            'min_trades': args.min_trades,
            'max_trades': args.max_trades
        }
        
        # Analyze labs
        lab_results = await analyzer.analyze_cached_labs(
            lab_ids=lab_ids,
            top_count=args.top_count,
            criteria=criteria
        )
        
        if not lab_results:
            logger.error("‚ùå No lab results generated")
            return 1
        
        # Show data distribution if requested
        if args.show_data_distribution:
            await analyzer.show_data_distribution(lab_results)
        
        # Generate reports if requested
        if args.generate_reports:
            await analyzer.generate_reports(
                lab_results, 
                criteria=criteria, 
                save_results=args.save_results
            )
        
        logger.info("‚úÖ Cache analysis completed successfully")
        return 0
        
    except Exception as e:
        logger.error(f"‚ùå Cache analysis failed: {e}")
        return 1


if __name__ == "__main__":
    exit(asyncio.run(main()))
