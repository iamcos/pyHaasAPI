"""
Walk Forward Optimization (WFO) Analysis for pyHaasAPI

This module provides comprehensive Walk Forward Optimization functionality
for analyzing trading strategies across different time periods and market conditions.
"""

import os
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum

# Optional imports for advanced analysis
try:
    import pandas as pd
    import numpy as np
    _has_pandas = True
except ImportError:
    _has_pandas = False
    # Create dummy classes for type hints
    class pd:
        class DataFrame:
            pass

from .. import api
from ..api import get_full_backtest_runtime_data
from .models import BacktestAnalysis
from .cache import UnifiedCacheManager

logger = logging.getLogger(__name__)


class WFOMode(Enum):
    """Walk Forward Optimization modes"""
    FIXED_WINDOW = "fixed_window"  # Fixed training and testing periods
    EXPANDING_WINDOW = "expanding_window"  # Expanding training window
    ROLLING_WINDOW = "rolling_window"  # Rolling training window


@dataclass
class WFOPeriod:
    """Represents a single WFO period"""
    period_id: int
    training_start: datetime
    training_end: datetime
    testing_start: datetime
    testing_end: datetime
    mode: WFOMode
    
    def __post_init__(self):
        """Validate period configuration"""
        if self.training_end >= self.testing_start:
            raise ValueError("Training period must end before testing period starts")
        if self.testing_start >= self.testing_end:
            raise ValueError("Testing start must be before testing end")


@dataclass
class WFOResult:
    """Results from a single WFO period"""
    period: WFOPeriod
    best_backtest_id: str
    best_parameters: Dict[str, Any]
    training_metrics: Dict[str, float]
    testing_metrics: Dict[str, float]
    out_of_sample_return: float
    out_of_sample_sharpe: float
    out_of_sample_max_drawdown: float
    stability_score: float
    success: bool
    error_message: Optional[str] = None


@dataclass
class WFOConfig:
    """Configuration for Walk Forward Optimization"""
    # Time period settings
    total_start_date: datetime
    total_end_date: datetime
    training_duration_days: int = 365  # 1 year training
    testing_duration_days: int = 90    # 3 months testing
    step_size_days: int = 30           # 1 month step
    
    # Optimization settings
    mode: WFOMode = WFOMode.ROLLING_WINDOW
    min_training_periods: int = 2
    max_optimization_iterations: int = 100
    
    # Performance criteria
    min_trades: int = 10
    min_win_rate: float = 0.4
    min_profit_factor: float = 1.1
    max_drawdown_threshold: float = 0.3
    
    # Risk management
    position_sizing_method: str = "fixed"  # fixed, kelly, volatility
    max_position_size: float = 0.1  # 10% of capital
    
    def validate(self) -> bool:
        """Validate WFO configuration"""
        if self.total_start_date >= self.total_end_date:
            raise ValueError("Start date must be before end date")
        if self.training_duration_days <= 0 or self.testing_duration_days <= 0:
            raise ValueError("Duration must be positive")
        if self.step_size_days <= 0:
            raise ValueError("Step size must be positive")
        return True


@dataclass
class WFOAnalysisResult:
    """Complete WFO analysis results"""
    lab_id: str
    config: WFOConfig
    periods: List[WFOPeriod]
    results: List[WFOResult]
    summary_metrics: Dict[str, float]
    stability_analysis: Dict[str, Any]
    parameter_evolution: Dict[str, List[Any]]
    performance_attribution: Dict[str, float]
    analysis_timestamp: str
    total_periods: int
    successful_periods: int
    failed_periods: int


class WFOAnalyzer:
    """Walk Forward Optimization Analyzer"""
    
    def __init__(self, cache_manager: Optional[UnifiedCacheManager] = None):
        self.cache_manager = cache_manager or UnifiedCacheManager()
        self.executor = None
    
    def connect(self, host: str = None, port: int = None, email: str = None, password: str = None) -> bool:
        """Connect to HaasOnline API"""
        try:
            from ..api import RequestsExecutor, Guest
            
            # Use environment variables if not provided
            host = host or os.getenv('API_HOST', '127.0.0.1')
            port = port or int(os.getenv('API_PORT', '8090'))
            email = email or os.getenv('API_EMAIL')
            password = password or os.getenv('API_PASSWORD')
            
            if not email or not password:
                logger.error("‚ùå Missing email or password in environment variables")
                return False
            
            # Create executor and authenticate
            executor = RequestsExecutor(host=host, port=port, state=Guest())
            self.executor = executor.authenticate(email, password)
            
            logger.info(f"‚úÖ Connected to HaasOnline API at {host}:{port}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Connection failed: {e}")
            return False
    
    def generate_wfo_periods(self, config: WFOConfig) -> List[WFOPeriod]:
        """Generate WFO periods based on configuration"""
        config.validate()
        
        periods = []
        current_date = config.total_start_date
        period_id = 0
        
        while current_date + timedelta(days=config.training_duration_days + config.testing_duration_days) <= config.total_end_date:
            # Define training period
            training_start = current_date
            training_end = current_date + timedelta(days=config.training_duration_days)
            
            # Define testing period (start one day after training ends)
            testing_start = training_end + timedelta(days=1)
            testing_end = testing_start + timedelta(days=config.testing_duration_days)
            
            # Create period
            period = WFOPeriod(
                period_id=period_id,
                training_start=training_start,
                training_end=training_end,
                testing_start=testing_start,
                testing_end=testing_end,
                mode=config.mode
            )
            periods.append(period)
            
            # Move to next period
            if config.mode == WFOMode.FIXED_WINDOW:
                current_date += timedelta(days=config.step_size_days)
            elif config.mode == WFOMode.ROLLING_WINDOW:
                current_date += timedelta(days=config.step_size_days)
            elif config.mode == WFOMode.EXPANDING_WINDOW:
                # For expanding window, training period grows but testing stays same size
                current_date += timedelta(days=config.step_size_days)
            
            period_id += 1
        
        logger.info(f"üìÖ Generated {len(periods)} WFO periods")
        return periods
    
    def analyze_wfo_period(self, lab_id: str, period: WFOPeriod, config: WFOConfig) -> WFOResult:
        """Analyze a single WFO period"""
        try:
            logger.info(f"üîç Analyzing WFO period {period.period_id}")
            logger.info(f"   Training: {period.training_start.date()} to {period.training_end.date()}")
            logger.info(f"   Testing: {period.testing_start.date()} to {period.testing_end.date()}")
            
            # Get all backtests for this lab
            backtests = api.get_backtest_result_page(self.executor, lab_id, 0, 1000)
            if not backtests or not hasattr(backtests, 'backtests'):
                raise ValueError("No backtests found")
            
            # Filter backtests by training period
            training_backtests = []
            for backtest in backtests.backtests:
                # Check if backtest falls within training period
                # Note: This is a simplified approach - in practice, you'd need to check actual backtest dates
                training_backtests.append(backtest)
            
            if len(training_backtests) < config.min_training_periods:
                raise ValueError(f"Insufficient training data: {len(training_backtests)} < {config.min_training_periods}")
            
            # Find best backtest in training period
            best_backtest = self._find_best_training_backtest(training_backtests, config)
            if not best_backtest:
                raise ValueError("No suitable backtest found in training period")
            
            # Extract training metrics
            training_metrics = self._extract_backtest_metrics(best_backtest)
            
            # Simulate out-of-sample testing
            # Note: In a real implementation, you'd run the strategy on the testing period
            testing_metrics = self._simulate_out_of_sample_performance(best_backtest, period, config)
            
            # Calculate stability score
            stability_score = self._calculate_stability_score(training_metrics, testing_metrics)
            
            return WFOResult(
                period=period,
                best_backtest_id=best_backtest.backtest_id,
                best_parameters=self._extract_parameters(best_backtest),
                training_metrics=training_metrics,
                testing_metrics=testing_metrics,
                out_of_sample_return=testing_metrics.get('return', 0.0),
                out_of_sample_sharpe=testing_metrics.get('sharpe_ratio', 0.0),
                out_of_sample_max_drawdown=testing_metrics.get('max_drawdown', 0.0),
                stability_score=stability_score,
                success=True
            )
            
        except Exception as e:
            logger.error(f"‚ùå Error analyzing WFO period {period.period_id}: {e}")
            return WFOResult(
                period=period,
                best_backtest_id="",
                best_parameters={},
                training_metrics={},
                testing_metrics={},
                out_of_sample_return=0.0,
                out_of_sample_sharpe=0.0,
                out_of_sample_max_drawdown=0.0,
                stability_score=0.0,
                success=False,
                error_message=str(e)
            )
    
    def _find_best_training_backtest(self, backtests: List[Any], config: WFOConfig) -> Optional[Any]:
        """Find the best backtest in the training period"""
        best_backtest = None
        best_score = float('-inf')
        
        for backtest in backtests:
            try:
                # Extract basic metrics
                roi = getattr(backtest, 'roi', 0.0)
                win_rate = getattr(backtest, 'win_rate', 0.0)
                total_trades = getattr(backtest, 'total_trades', 0)
                max_drawdown = getattr(backtest, 'max_drawdown', 0.0)
                
                # Apply filters
                if total_trades < config.min_trades:
                    continue
                if win_rate < config.min_win_rate:
                    continue
                if max_drawdown > config.max_drawdown_threshold:
                    continue
                
                # Calculate composite score
                score = self._calculate_composite_score(roi, win_rate, total_trades, max_drawdown)
                
                if score > best_score:
                    best_score = score
                    best_backtest = backtest
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error processing backtest: {e}")
                continue
        
        return best_backtest
    
    def _calculate_composite_score(self, roi: float, win_rate: float, total_trades: int, max_drawdown: float) -> float:
        """Calculate composite performance score"""
        # Normalize metrics
        roi_score = min(roi / 1000.0, 5.0)  # Cap ROI at 5000%
        win_rate_score = win_rate * 10  # Scale win rate
        trades_score = min(total_trades / 100.0, 2.0)  # Cap trades score
        
        # Normalize drawdown penalty (drawdowns can be very high, so use logarithmic penalty)
        if max_drawdown > 0:
            # Use a more reasonable penalty that doesn't dominate the score
            drawdown_penalty = min(max_drawdown * 0.01, 10)  # Cap penalty at 10
        else:
            drawdown_penalty = 0
        
        # Weighted composite score
        score = (roi_score * 0.4 + win_rate_score * 0.3 + trades_score * 0.2) - drawdown_penalty
        return score
    
    def _extract_backtest_metrics(self, backtest: Any) -> Dict[str, float]:
        """Extract metrics from a backtest"""
        return {
            'roi': getattr(backtest, 'roi', 0.0),
            'win_rate': getattr(backtest, 'win_rate', 0.0),
            'total_trades': getattr(backtest, 'total_trades', 0),
            'max_drawdown': getattr(backtest, 'max_drawdown', 0.0),
            'profit_factor': getattr(backtest, 'profit_factor', 0.0),
            'sharpe_ratio': getattr(backtest, 'sharpe_ratio', 0.0),
            'avg_profit_per_trade': getattr(backtest, 'avg_profit_per_trade', 0.0)
        }
    
    def _extract_parameters(self, backtest: Any) -> Dict[str, Any]:
        """Extract parameters from a backtest"""
        # This would extract the actual parameters used in the backtest
        # For now, return a placeholder
        return {
            'backtest_id': getattr(backtest, 'backtest_id', ''),
            'generation_idx': getattr(backtest, 'generation_idx', 0),
            'population_idx': getattr(backtest, 'population_idx', 0)
        }
    
    def _simulate_out_of_sample_performance(self, backtest: Any, period: WFOPeriod, config: WFOConfig) -> Dict[str, float]:
        """Simulate out-of-sample performance"""
        # This is a simplified simulation
        # In practice, you'd run the strategy on the actual testing period
        
        base_metrics = self._extract_backtest_metrics(backtest)
        
        if _has_pandas:
            # Add some randomness to simulate out-of-sample performance
            np.random.seed(42)  # For reproducibility
            
            # Simulate performance degradation
            degradation_factor = np.random.uniform(0.7, 1.0)
            
            simulated_metrics = {
                'return': base_metrics['roi'] * degradation_factor,
                'win_rate': max(0.0, base_metrics['win_rate'] - np.random.uniform(0.0, 0.1)),
                'sharpe_ratio': base_metrics['sharpe_ratio'] * degradation_factor,
                'max_drawdown': base_metrics['max_drawdown'] + np.random.uniform(0.0, base_metrics['max_drawdown'] * 0.05),
                'profit_factor': base_metrics['profit_factor'] * degradation_factor
            }
        else:
            # Fallback without numpy
            import random
            random.seed(42)
            
            # Simulate performance degradation
            degradation_factor = random.uniform(0.7, 1.0)
            
            simulated_metrics = {
                'return': base_metrics['roi'] * degradation_factor,
                'win_rate': max(0.0, base_metrics['win_rate'] - random.uniform(0.0, 0.1)),
                'sharpe_ratio': base_metrics['sharpe_ratio'] * degradation_factor,
                'max_drawdown': base_metrics['max_drawdown'] + random.uniform(0.0, base_metrics['max_drawdown'] * 0.05),
                'profit_factor': base_metrics['profit_factor'] * degradation_factor
            }
        
        return simulated_metrics
    
    def _calculate_stability_score(self, training_metrics: Dict[str, float], testing_metrics: Dict[str, float]) -> float:
        """Calculate stability score between training and testing performance"""
        if not training_metrics or not testing_metrics:
            return 0.0
        
        # Calculate relative performance degradation
        roi_degradation = abs(training_metrics.get('roi', 0) - testing_metrics.get('return', 0)) / max(training_metrics.get('roi', 1), 1)
        win_rate_degradation = abs(training_metrics.get('win_rate', 0) - testing_metrics.get('win_rate', 0))
        
        # Stability score (higher is better)
        stability = 1.0 - (roi_degradation + win_rate_degradation) / 2.0
        return max(0.0, min(1.0, stability))
    
    def analyze_lab_wfo(self, lab_id: str, config: WFOConfig) -> WFOAnalysisResult:
        """Perform complete WFO analysis on a lab"""
        logger.info(f"üöÄ Starting WFO analysis for lab {lab_id[:8]}")
        
        # Generate periods
        periods = self.generate_wfo_periods(config)
        if not periods:
            raise ValueError("No valid WFO periods generated")
        
        # Analyze each period
        results = []
        for period in periods:
            result = self.analyze_wfo_period(lab_id, period, config)
            results.append(result)
        
        # Calculate summary metrics
        summary_metrics = self._calculate_summary_metrics(results)
        
        # Perform stability analysis
        stability_analysis = self._analyze_stability(results)
        
        # Analyze parameter evolution
        parameter_evolution = self._analyze_parameter_evolution(results)
        
        # Performance attribution
        performance_attribution = self._analyze_performance_attribution(results)
        
        # Count successful vs failed periods
        successful_periods = sum(1 for r in results if r.success)
        failed_periods = len(results) - successful_periods
        
        return WFOAnalysisResult(
            lab_id=lab_id,
            config=config,
            periods=periods,
            results=results,
            summary_metrics=summary_metrics,
            stability_analysis=stability_analysis,
            parameter_evolution=parameter_evolution,
            performance_attribution=performance_attribution,
            analysis_timestamp=datetime.now().isoformat(),
            total_periods=len(periods),
            successful_periods=successful_periods,
            failed_periods=failed_periods
        )
    
    def _calculate_summary_metrics(self, results: List[WFOResult]) -> Dict[str, float]:
        """Calculate summary metrics across all WFO periods"""
        successful_results = [r for r in results if r.success]
        
        if not successful_results:
            return {}
        
        returns = [r.out_of_sample_return for r in successful_results]
        sharpes = [r.out_of_sample_sharpe for r in successful_results]
        drawdowns = [r.out_of_sample_max_drawdown for r in successful_results]
        stability_scores = [r.stability_score for r in successful_results]
        
        if _has_pandas:
            return {
                'avg_return': np.mean(returns),
                'median_return': np.median(returns),
                'std_return': np.std(returns),
                'avg_sharpe': np.mean(sharpes),
                'median_sharpe': np.median(sharpes),
                'avg_drawdown': np.mean(drawdowns),
                'max_drawdown': np.max(drawdowns),
                'avg_stability': np.mean(stability_scores),
                'consistency_ratio': len([r for r in returns if r > 0]) / len(returns),
                'total_periods': len(results),
                'successful_periods': len(successful_results),
                'success_rate': len(successful_results) / len(results)
            }
        else:
            # Fallback to basic statistics without numpy
            return {
                'avg_return': sum(returns) / len(returns),
                'median_return': sorted(returns)[len(returns)//2],
                'std_return': (sum((x - sum(returns)/len(returns))**2 for x in returns) / len(returns))**0.5,
                'avg_sharpe': sum(sharpes) / len(sharpes),
                'median_sharpe': sorted(sharpes)[len(sharpes)//2],
                'avg_drawdown': sum(drawdowns) / len(drawdowns),
                'max_drawdown': max(drawdowns),
                'avg_stability': sum(stability_scores) / len(stability_scores),
                'consistency_ratio': len([r for r in returns if r > 0]) / len(returns),
                'total_periods': len(results),
                'successful_periods': len(successful_results),
                'success_rate': len(successful_results) / len(results)
            }
    
    def _analyze_stability(self, results: List[WFOResult]) -> Dict[str, Any]:
        """Analyze stability across WFO periods"""
        successful_results = [r for r in results if r.success]
        
        if len(successful_results) < 2:
            return {'insufficient_data': True}
        
        returns = [r.out_of_sample_return for r in successful_results]
        stability_scores = [r.stability_score for r in successful_results]
        
        if _has_pandas:
            return {
                'return_volatility': np.std(returns),
                'return_consistency': 1.0 - (np.std(returns) / max(np.mean(returns), 0.01)),
                'avg_stability_score': np.mean(stability_scores),
                'stability_trend': self._calculate_trend(stability_scores),
                'performance_degradation': self._calculate_performance_degradation(returns)
            }
        else:
            # Fallback without numpy
            mean_return = sum(returns) / len(returns)
            std_return = (sum((x - mean_return)**2 for x in returns) / len(returns))**0.5
            return {
                'return_volatility': std_return,
                'return_consistency': 1.0 - (std_return / max(mean_return, 0.01)),
                'avg_stability_score': sum(stability_scores) / len(stability_scores),
                'stability_trend': self._calculate_trend(stability_scores),
                'performance_degradation': self._calculate_performance_degradation(returns)
            }
    
    def _analyze_parameter_evolution(self, results: List[WFOResult]) -> Dict[str, List[Any]]:
        """Analyze how parameters evolve across WFO periods"""
        # This would track how the best parameters change over time
        # For now, return a placeholder structure
        return {
            'parameter_changes': [],
            'stability_metrics': {},
            'evolution_patterns': {}
        }
    
    def _analyze_performance_attribution(self, results: List[WFOResult]) -> Dict[str, float]:
        """Analyze performance attribution factors"""
        successful_results = [r for r in results if r.success]
        
        if not successful_results:
            return {}
        
        # Analyze what drives performance
        returns = [r.out_of_sample_return for r in successful_results]
        sharpes = [r.out_of_sample_sharpe for r in successful_results]
        drawdowns = [r.out_of_sample_max_drawdown for r in successful_results]
        
        if _has_pandas:
            return {
                'return_contribution': np.mean(returns),
                'risk_adjusted_contribution': np.mean(sharpes),
                'drawdown_impact': -np.mean(drawdowns),
                'risk_return_ratio': np.mean(sharpes) / max(np.std(returns), 0.01)
            }
        else:
            # Fallback without numpy
            mean_return = sum(returns) / len(returns)
            mean_sharpe = sum(sharpes) / len(sharpes)
            mean_drawdown = sum(drawdowns) / len(drawdowns)
            std_return = (sum((x - mean_return)**2 for x in returns) / len(returns))**0.5
            
            return {
                'return_contribution': mean_return,
                'risk_adjusted_contribution': mean_sharpe,
                'drawdown_impact': -mean_drawdown,
                'risk_return_ratio': mean_sharpe / max(std_return, 0.01)
            }
    
    def _calculate_trend(self, values: List[float]) -> str:
        """Calculate trend direction"""
        if len(values) < 2:
            return "insufficient_data"
        
        if _has_pandas:
            # Simple linear trend with numpy
            x = np.arange(len(values))
            slope = np.polyfit(x, values, 1)[0]
        else:
            # Simple linear trend without numpy
            n = len(values)
            x = list(range(n))
            x_mean = sum(x) / n
            y_mean = sum(values) / n
            
            numerator = sum((x[i] - x_mean) * (values[i] - y_mean) for i in range(n))
            denominator = sum((x[i] - x_mean) ** 2 for i in range(n))
            slope = numerator / denominator if denominator != 0 else 0
        
        if slope > 0.01:
            return "improving"
        elif slope < -0.01:
            return "declining"
        else:
            return "stable"
    
    def _calculate_performance_degradation(self, returns: List[float]) -> float:
        """Calculate performance degradation over time"""
        if len(returns) < 2:
            return 0.0
        
        # Compare first half vs second half
        mid_point = len(returns) // 2
        first_half = returns[:mid_point]
        second_half = returns[mid_point:]
        
        if not first_half or not second_half:
            return 0.0
        
        if _has_pandas:
            first_half_avg = np.mean(first_half)
            second_half_avg = np.mean(second_half)
        else:
            first_half_avg = sum(first_half) / len(first_half)
            second_half_avg = sum(second_half) / len(second_half)
        
        if first_half_avg == 0:
            return 0.0
        
        degradation = (first_half_avg - second_half_avg) / abs(first_half_avg)
        return degradation
    
    def save_wfo_report(self, result: WFOAnalysisResult, output_path: str = None) -> str:
        """Save WFO analysis report to CSV"""
        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"wfo_analysis_{result.lab_id[:8]}_{timestamp}.csv"
        
        # Prepare data for CSV
        rows = []
        for wfo_result in result.results:
            row = {
                'period_id': wfo_result.period.period_id,
                'training_start': wfo_result.period.training_start.isoformat(),
                'training_end': wfo_result.period.training_end.isoformat(),
                'testing_start': wfo_result.period.testing_start.isoformat(),
                'testing_end': wfo_result.period.testing_end.isoformat(),
                'best_backtest_id': wfo_result.best_backtest_id,
                'out_of_sample_return': wfo_result.out_of_sample_return,
                'out_of_sample_sharpe': wfo_result.out_of_sample_sharpe,
                'out_of_sample_max_drawdown': wfo_result.out_of_sample_max_drawdown,
                'stability_score': wfo_result.stability_score,
                'success': wfo_result.success,
                'error_message': wfo_result.error_message or ''
            }
            
            # Add training metrics
            for key, value in wfo_result.training_metrics.items():
                row[f'training_{key}'] = value
            
            # Add testing metrics
            for key, value in wfo_result.testing_metrics.items():
                row[f'testing_{key}'] = value
            
            rows.append(row)
        
        if _has_pandas:
            # Create DataFrame and save with pandas
            df = pd.DataFrame(rows)
            df.to_csv(output_path, index=False)
        else:
            # Save CSV manually without pandas
            import csv
            if rows:
                with open(output_path, 'w', newline='') as csvfile:
                    fieldnames = rows[0].keys()
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(rows)
        
        logger.info(f"üìä WFO report saved: {output_path}")
        return output_path
