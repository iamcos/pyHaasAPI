"""
Walk Forward Optimization (WFO) Analysis for pyHaasAPI v2

This module provides comprehensive Walk Forward Optimization functionality
for analyzing trading strategies across different time periods and market conditions.

Migrated from pyHaasAPI_v1/analysis/wfo.py and adapted for async v2 architecture.
"""

import os
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum

# Optional imports for advanced analysis
try:
    import pandas as pd
    import numpy as np
    _has_pandas = True
except ImportError:
    _has_pandas = False

from ..core.logging import get_logger

logger = get_logger("wfo")


class WFOMode(Enum):
    """Walk Forward Optimization modes"""
    FIXED_WINDOW = "fixed_window"  # Fixed training and testing periods
    EXPANDING_WINDOW = "expanding_window"  # Expanding training window
    ROLLING_WINDOW = "rolling_window"  # Rolling training window


@dataclass
class WFOPeriod:
    """Represents a single WFO period"""
    period_id: int
    training_start: datetime
    training_end: datetime
    testing_start: datetime
    testing_end: datetime
    mode: WFOMode
    
    def __post_init__(self):
        """Validate period configuration"""
        if self.training_end >= self.testing_start:
            raise ValueError("Training period must end before testing period starts")
        if self.testing_start >= self.testing_end:
            raise ValueError("Testing start must be before testing end")


@dataclass
class WFOResult:
    """Results from a single WFO period"""
    period: WFOPeriod
    best_backtest_id: str
    best_parameters: Dict[str, Any]
    training_metrics: Dict[str, float]
    testing_metrics: Dict[str, float]
    out_of_sample_return: float
    out_of_sample_sharpe: float
    out_of_sample_max_drawdown: float
    stability_score: float
    success: bool
    error_message: Optional[str] = None


@dataclass
class WFOConfig:
    """Configuration for Walk Forward Optimization"""
    # Time period settings
    total_start_date: datetime
    total_end_date: datetime
    training_duration_days: int = 365  # 1 year training
    testing_duration_days: int = 90    # 3 months testing
    step_size_days: int = 30           # 1 month step
    
    # Optimization settings
    mode: WFOMode = WFOMode.ROLLING_WINDOW
    min_training_periods: int = 2
    max_optimization_iterations: int = 100
    
    # Performance criteria
    min_trades: int = 10
    min_win_rate: float = 0.4
    min_profit_factor: float = 1.1
    max_drawdown_threshold: float = 0.3
    
    # Risk management
    position_sizing_method: str = "fixed"  # fixed, kelly, volatility
    max_position_size: float = 0.1  # 10% of capital
    
    def validate(self) -> bool:
        """Validate WFO configuration"""
        if self.total_start_date >= self.total_end_date:
            raise ValueError("Start date must be before end date")
        if self.training_duration_days <= 0 or self.testing_duration_days <= 0:
            raise ValueError("Duration must be positive")
        if self.step_size_days <= 0:
            raise ValueError("Step size must be positive")
        return True


@dataclass
class WFOAnalysisResult:
    """Complete WFO analysis results"""
    lab_id: str
    config: WFOConfig
    periods: List[WFOPeriod]
    results: List[WFOResult]
    summary_metrics: Dict[str, float]
    stability_analysis: Dict[str, Any]
    parameter_evolution: Dict[str, List[Any]]
    performance_attribution: Dict[str, float]
    analysis_timestamp: str
    total_periods: int
    successful_periods: int
    failed_periods: int
    
    # Additional fields for v2 compatibility
    average_period_roi: float = 0.0
    best_period_roi: float = 0.0
    worst_period_roi: float = 0.0
    stability_score: float = 0.0
    out_of_sample_performance: float = 0.0
    recommendations: List[str] = None
    
    def __post_init__(self):
        """Calculate derived metrics"""
        if self.results:
            successful_results = [r for r in self.results if r.success]
            if successful_results:
                returns = [r.out_of_sample_return for r in successful_results]
                self.average_period_roi = sum(returns) / len(returns) if returns else 0.0
                self.best_period_roi = max(returns) if returns else 0.0
                self.worst_period_roi = min(returns) if returns else 0.0
                self.out_of_sample_performance = self.average_period_roi
                
                stability_scores = [r.stability_score for r in successful_results]
                self.stability_score = sum(stability_scores) / len(stability_scores) if stability_scores else 0.0
                
                # Generate recommendations
                if self.recommendations is None:
                    self.recommendations = self._generate_recommendations()
    
    def _generate_recommendations(self) -> List[str]:
        """Generate recommendations based on analysis results"""
        recommendations = []
        
        if self.successful_periods / max(self.total_periods, 1) < 0.7:
            recommendations.append("Low success rate: Consider adjusting performance criteria or time periods")
        
        if self.stability_score < 0.5:
            recommendations.append("Low stability: Strategy performance varies significantly across periods")
        
        if self.average_period_roi < 0:
            recommendations.append("Negative average return: Strategy may not be suitable for live trading")
        
        if self.successful_periods > 0:
            recommendations.append(f"Strategy shows promise with {self.successful_periods}/{self.total_periods} successful periods")
        
        return recommendations if recommendations else ["Analysis complete - review results for detailed insights"]


class WFOAnalyzer:
    """Walk Forward Optimization Analyzer (Async v2 version)"""
    
    def __init__(self, lab_api=None, backtest_api=None):
        """
        Initialize WFO Analyzer
        
        Args:
            lab_api: LabAPI instance for lab operations
            backtest_api: BacktestAPI instance for backtest operations
        """
        self.lab_api = lab_api
        self.backtest_api = backtest_api
        self.logger = get_logger("wfo_analyzer")
    
    def generate_wfo_periods(self, config: WFOConfig) -> List[WFOPeriod]:
        """Generate WFO periods based on configuration"""
        config.validate()
        
        periods = []
        current_date = config.total_start_date
        period_id = 0
        
        while current_date + timedelta(days=config.training_duration_days + config.testing_duration_days) <= config.total_end_date:
            # Define training period
            training_start = current_date
            training_end = current_date + timedelta(days=config.training_duration_days)
            
            # Define testing period (start one day after training ends)
            testing_start = training_end + timedelta(days=1)
            testing_end = testing_start + timedelta(days=config.testing_duration_days)
            
            # Create period
            period = WFOPeriod(
                period_id=period_id,
                training_start=training_start,
                training_end=training_end,
                testing_start=testing_start,
                testing_end=testing_end,
                mode=config.mode
            )
            periods.append(period)
            
            # Move to next period
            if config.mode == WFOMode.FIXED_WINDOW:
                current_date += timedelta(days=config.step_size_days)
            elif config.mode == WFOMode.ROLLING_WINDOW:
                current_date += timedelta(days=config.step_size_days)
            elif config.mode == WFOMode.EXPANDING_WINDOW:
                # For expanding window, training period grows but testing stays same size
                current_date += timedelta(days=config.step_size_days)
            
            period_id += 1
        
        self.logger.info(f"üìÖ Generated {len(periods)} WFO periods")
        return periods
    
    async def analyze_wfo_period(
        self, 
        lab_id: str, 
        period: WFOPeriod, 
        config: WFOConfig
    ) -> WFOResult:
        """Analyze a single WFO period (async)"""
        try:
            self.logger.info(f"üîç Analyzing WFO period {period.period_id}")
            self.logger.info(f"   Training: {period.training_start.date()} to {period.training_end.date()}")
            self.logger.info(f"   Testing: {period.testing_start.date()} to {period.testing_end.date()}")
            
            # Get all backtests for this lab
            if not self.backtest_api:
                raise ValueError("BacktestAPI not initialized")
            
            backtest_results = await self.backtest_api.get_all_backtests_for_lab(lab_id)
            
            if not backtest_results:
                raise ValueError("No backtests found")
            
            # Filter backtests by training period (simplified - check by generation/population indices)
            training_backtests = []
            for backtest in backtest_results:
                # In a real implementation, we'd filter by actual backtest dates
                # For now, include all backtests and filter by criteria
                training_backtests.append(backtest)
            
            if len(training_backtests) < config.min_training_periods:
                raise ValueError(f"Insufficient training data: {len(training_backtests)} < {config.min_training_periods}")
            
            # Find best backtest in training period
            best_backtest = self._find_best_training_backtest(training_backtests, config)
            if not best_backtest:
                raise ValueError("No suitable backtest found in training period")
            
            # Extract training metrics
            training_metrics = self._extract_backtest_metrics(best_backtest)
            
            # Simulate out-of-sample testing
            testing_metrics = self._simulate_out_of_sample_performance(best_backtest, period, config)
            
            # Calculate stability score
            stability_score = self._calculate_stability_score(training_metrics, testing_metrics)
            
            return WFOResult(
                period=period,
                best_backtest_id=getattr(best_backtest, 'backtest_id', ''),
                best_parameters=self._extract_parameters(best_backtest),
                training_metrics=training_metrics,
                testing_metrics=testing_metrics,
                out_of_sample_return=testing_metrics.get('return', 0.0),
                out_of_sample_sharpe=testing_metrics.get('sharpe_ratio', 0.0),
                out_of_sample_max_drawdown=testing_metrics.get('max_drawdown', 0.0),
                stability_score=stability_score,
                success=True
            )
            
        except Exception as e:
            self.logger.error(f"‚ùå Error analyzing WFO period {period.period_id}: {e}")
            return WFOResult(
                period=period,
                best_backtest_id="",
                best_parameters={},
                training_metrics={},
                testing_metrics={},
                out_of_sample_return=0.0,
                out_of_sample_sharpe=0.0,
                out_of_sample_max_drawdown=0.0,
                stability_score=0.0,
                success=False,
                error_message=str(e)
            )
    
    def _find_best_training_backtest(self, backtests: List[Any], config: WFOConfig) -> Optional[Any]:
        """Find the best backtest in the training period"""
        best_backtest = None
        best_score = float('-inf')
        
        for backtest in backtests:
            try:
                # Extract basic metrics using safe field access
                roi = getattr(backtest, 'roi_percentage', 0.0)
                if not roi:
                    roi = getattr(backtest, 'roi', 0.0)
                
                win_rate = getattr(backtest, 'win_rate', 0.0)
                total_trades = getattr(backtest, 'total_trades', 0)
                max_drawdown = getattr(backtest, 'max_drawdown', 0.0)
                
                # Apply filters
                if total_trades < config.min_trades:
                    continue
                if win_rate < config.min_win_rate:
                    continue
                if max_drawdown > config.max_drawdown_threshold:
                    continue
                
                # Calculate composite score
                score = self._calculate_composite_score(roi, win_rate, total_trades, max_drawdown)
                
                if score > best_score:
                    best_score = score
                    best_backtest = backtest
                    
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Error processing backtest: {e}")
                continue
        
        return best_backtest
    
    def _calculate_composite_score(self, roi: float, win_rate: float, total_trades: int, max_drawdown: float) -> float:
        """Calculate composite performance score"""
        # Normalize metrics
        roi_score = min(roi / 1000.0, 5.0)  # Cap ROI at 5000%
        win_rate_score = win_rate * 10  # Scale win rate
        trades_score = min(total_trades / 100.0, 2.0)  # Cap trades score
        
        # Normalize drawdown penalty
        if max_drawdown > 0:
            drawdown_penalty = min(max_drawdown * 0.01, 10)  # Cap penalty at 10
        else:
            drawdown_penalty = 0
        
        # Weighted composite score
        score = (roi_score * 0.4 + win_rate_score * 0.3 + trades_score * 0.2) - drawdown_penalty
        return score
    
    def _extract_backtest_metrics(self, backtest: Any) -> Dict[str, float]:
        """Extract metrics from a backtest"""
        return {
            'roi': getattr(backtest, 'roi_percentage', getattr(backtest, 'roi', 0.0)),
            'win_rate': getattr(backtest, 'win_rate', 0.0),
            'total_trades': getattr(backtest, 'total_trades', 0),
            'max_drawdown': getattr(backtest, 'max_drawdown', 0.0),
            'profit_factor': getattr(backtest, 'profit_factor', 0.0),
            'sharpe_ratio': getattr(backtest, 'sharpe_ratio', 0.0),
            'avg_profit_per_trade': getattr(backtest, 'avg_profit_per_trade', 0.0)
        }
    
    def _extract_parameters(self, backtest: Any) -> Dict[str, Any]:
        """Extract parameters from a backtest"""
        return {
            'backtest_id': getattr(backtest, 'backtest_id', ''),
            'generation_idx': getattr(backtest, 'generation_idx', 0),
            'population_idx': getattr(backtest, 'population_idx', 0)
        }
    
    def _simulate_out_of_sample_performance(self, backtest: Any, period: WFOPeriod, config: WFOConfig) -> Dict[str, float]:
        """Simulate out-of-sample performance"""
        base_metrics = self._extract_backtest_metrics(backtest)
        
        if _has_pandas:
            # Add some randomness to simulate out-of-sample performance
            np.random.seed(42)  # For reproducibility
            
            # Simulate performance degradation
            degradation_factor = np.random.uniform(0.7, 1.0)
            
            simulated_metrics = {
                'return': base_metrics['roi'] * degradation_factor,
                'win_rate': max(0.0, base_metrics['win_rate'] - np.random.uniform(0.0, 0.1)),
                'sharpe_ratio': base_metrics['sharpe_ratio'] * degradation_factor,
                'max_drawdown': base_metrics['max_drawdown'] + np.random.uniform(0.0, base_metrics['max_drawdown'] * 0.05),
                'profit_factor': base_metrics['profit_factor'] * degradation_factor
            }
        else:
            # Fallback without numpy
            import random
            random.seed(42)
            
            # Simulate performance degradation
            degradation_factor = random.uniform(0.7, 1.0)
            
            simulated_metrics = {
                'return': base_metrics['roi'] * degradation_factor,
                'win_rate': max(0.0, base_metrics['win_rate'] - random.uniform(0.0, 0.1)),
                'sharpe_ratio': base_metrics['sharpe_ratio'] * degradation_factor,
                'max_drawdown': base_metrics['max_drawdown'] + random.uniform(0.0, base_metrics['max_drawdown'] * 0.05),
                'profit_factor': base_metrics['profit_factor'] * degradation_factor
            }
        
        return simulated_metrics
    
    def _calculate_stability_score(self, training_metrics: Dict[str, float], testing_metrics: Dict[str, float]) -> float:
        """Calculate stability score between training and testing performance"""
        if not training_metrics or not testing_metrics:
            return 0.0
        
        # Calculate relative performance degradation
        training_roi = training_metrics.get('roi', 0)
        testing_return = testing_metrics.get('return', 0)
        
        if training_roi == 0:
            return 0.0
        
        roi_degradation = abs(training_roi - testing_return) / max(abs(training_roi), 1)
        win_rate_degradation = abs(training_metrics.get('win_rate', 0) - testing_metrics.get('win_rate', 0))
        
        # Stability score (higher is better)
        stability = 1.0 - (roi_degradation + win_rate_degradation) / 2.0
        return max(0.0, min(1.0, stability))
    
    async def analyze_lab_wfo(self, lab_id: str, config: WFOConfig) -> WFOAnalysisResult:
        """Perform complete WFO analysis on a lab (async)"""
        self.logger.info(f"üöÄ Starting WFO analysis for lab {lab_id[:8]}")
        
        # Generate periods
        periods = self.generate_wfo_periods(config)
        if not periods:
            raise ValueError("No valid WFO periods generated")
        
        # Analyze each period
        results = []
        for period in periods:
            result = await self.analyze_wfo_period(lab_id, period, config)
            results.append(result)
        
        # Calculate summary metrics
        summary_metrics = self._calculate_summary_metrics(results)
        
        # Perform stability analysis
        stability_analysis = self._analyze_stability(results)
        
        # Analyze parameter evolution
        parameter_evolution = self._analyze_parameter_evolution(results)
        
        # Performance attribution
        performance_attribution = self._analyze_performance_attribution(results)
        
        # Count successful vs failed periods
        successful_periods = sum(1 for r in results if r.success)
        failed_periods = len(results) - successful_periods
        
        result = WFOAnalysisResult(
            lab_id=lab_id,
            config=config,
            periods=periods,
            results=results,
            summary_metrics=summary_metrics,
            stability_analysis=stability_analysis,
            parameter_evolution=parameter_evolution,
            performance_attribution=performance_attribution,
            analysis_timestamp=datetime.now().isoformat(),
            total_periods=len(periods),
            successful_periods=successful_periods,
            failed_periods=failed_periods
        )
        
        return result
    
    def _calculate_summary_metrics(self, results: List[WFOResult]) -> Dict[str, float]:
        """Calculate summary metrics across all WFO periods"""
        successful_results = [r for r in results if r.success]
        
        if not successful_results:
            return {}
        
        returns = [r.out_of_sample_return for r in successful_results]
        sharpes = [r.out_of_sample_sharpe for r in successful_results]
        drawdowns = [r.out_of_sample_max_drawdown for r in successful_results]
        stability_scores = [r.stability_score for r in successful_results]
        
        if _has_pandas:
            return {
                'avg_return': np.mean(returns),
                'median_return': np.median(returns),
                'std_return': np.std(returns),
                'avg_sharpe': np.mean(sharpes),
                'median_sharpe': np.median(sharpes),
                'avg_drawdown': np.mean(drawdowns),
                'max_drawdown': np.max(drawdowns),
                'avg_stability': np.mean(stability_scores),
                'consistency_ratio': len([r for r in returns if r > 0]) / len(returns),
                'total_periods': len(results),
                'successful_periods': len(successful_results),
                'success_rate': len(successful_results) / len(results) if results else 0
            }
        else:
            # Fallback to basic statistics without numpy
            return {
                'avg_return': sum(returns) / len(returns),
                'median_return': sorted(returns)[len(returns)//2],
                'std_return': (sum((x - sum(returns)/len(returns))**2 for x in returns) / len(returns))**0.5,
                'avg_sharpe': sum(sharpes) / len(sharpes),
                'median_sharpe': sorted(sharpes)[len(sharpes)//2],
                'avg_drawdown': sum(drawdowns) / len(drawdowns),
                'max_drawdown': max(drawdowns),
                'avg_stability': sum(stability_scores) / len(stability_scores),
                'consistency_ratio': len([r for r in returns if r > 0]) / len(returns),
                'total_periods': len(results),
                'successful_periods': len(successful_results),
                'success_rate': len(successful_results) / len(results) if results else 0
            }
    
    def _analyze_stability(self, results: List[WFOResult]) -> Dict[str, Any]:
        """Analyze stability across WFO periods"""
        successful_results = [r for r in results if r.success]
        
        if len(successful_results) < 2:
            return {'insufficient_data': True}
        
        returns = [r.out_of_sample_return for r in successful_results]
        stability_scores = [r.stability_score for r in successful_results]
        
        if _has_pandas:
            return {
                'return_volatility': np.std(returns),
                'return_consistency': 1.0 - (np.std(returns) / max(np.mean(returns), 0.01)),
                'avg_stability_score': np.mean(stability_scores),
                'stability_trend': self._calculate_trend(stability_scores),
                'performance_degradation': self._calculate_performance_degradation(returns)
            }
        else:
            # Fallback without numpy
            mean_return = sum(returns) / len(returns)
            std_return = (sum((x - mean_return)**2 for x in returns) / len(returns))**0.5
            return {
                'return_volatility': std_return,
                'return_consistency': 1.0 - (std_return / max(mean_return, 0.01)),
                'avg_stability_score': sum(stability_scores) / len(stability_scores),
                'stability_trend': self._calculate_trend(stability_scores),
                'performance_degradation': self._calculate_performance_degradation(returns)
            }
    
    def _analyze_parameter_evolution(self, results: List[WFOResult]) -> Dict[str, List[Any]]:
        """Analyze how parameters evolve across WFO periods"""
        # This would track how the best parameters change over time
        # For now, return a placeholder structure
        return {
            'parameter_changes': [],
            'stability_metrics': {},
            'evolution_patterns': {}
        }
    
    def _analyze_performance_attribution(self, results: List[WFOResult]) -> Dict[str, float]:
        """Analyze performance attribution factors"""
        successful_results = [r for r in results if r.success]
        
        if not successful_results:
            return {}
        
        # Analyze what drives performance
        returns = [r.out_of_sample_return for r in successful_results]
        sharpes = [r.out_of_sample_sharpe for r in successful_results]
        drawdowns = [r.out_of_sample_max_drawdown for r in successful_results]
        
        if _has_pandas:
            return {
                'return_contribution': np.mean(returns),
                'risk_adjusted_contribution': np.mean(sharpes),
                'drawdown_impact': -np.mean(drawdowns),
                'risk_return_ratio': np.mean(sharpes) / max(np.std(returns), 0.01)
            }
        else:
            # Fallback without numpy
            mean_return = sum(returns) / len(returns)
            mean_sharpe = sum(sharpes) / len(sharpes)
            mean_drawdown = sum(drawdowns) / len(drawdowns)
            std_return = (sum((x - mean_return)**2 for x in returns) / len(returns))**0.5
            
            return {
                'return_contribution': mean_return,
                'risk_adjusted_contribution': mean_sharpe,
                'drawdown_impact': -mean_drawdown,
                'risk_return_ratio': mean_sharpe / max(std_return, 0.01)
            }
    
    def _calculate_trend(self, values: List[float]) -> str:
        """Calculate trend direction"""
        if len(values) < 2:
            return "insufficient_data"
        
        if _has_pandas:
            # Simple linear trend with numpy
            x = np.arange(len(values))
            slope = np.polyfit(x, values, 1)[0]
        else:
            # Simple linear trend without numpy
            n = len(values)
            x = list(range(n))
            x_mean = sum(x) / n
            y_mean = sum(values) / n
            
            numerator = sum((x[i] - x_mean) * (values[i] - y_mean) for i in range(n))
            denominator = sum((x[i] - x_mean) ** 2 for i in range(n))
            slope = numerator / denominator if denominator != 0 else 0
        
        if slope > 0.01:
            return "improving"
        elif slope < -0.01:
            return "declining"
        else:
            return "stable"
    
    def _calculate_performance_degradation(self, returns: List[float]) -> float:
        """Calculate performance degradation over time"""
        if len(returns) < 2:
            return 0.0
        
        # Compare first half vs second half
        mid_point = len(returns) // 2
        first_half = returns[:mid_point]
        second_half = returns[mid_point:]
        
        if not first_half or not second_half:
            return 0.0
        
        if _has_pandas:
            first_half_avg = np.mean(first_half)
            second_half_avg = np.mean(second_half)
        else:
            first_half_avg = sum(first_half) / len(first_half)
            second_half_avg = sum(second_half) / len(second_half)
        
        if first_half_avg == 0:
            return 0.0
        
        degradation = (first_half_avg - second_half_avg) / abs(first_half_avg)
        return degradation
    
    def save_wfo_report(self, result: WFOAnalysisResult, output_path: str = None) -> str:
        """Save WFO analysis report to CSV"""
        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"wfo_analysis_{result.lab_id[:8]}_{timestamp}.csv"
        
        # Prepare data for CSV
        rows = []
        for wfo_result in result.results:
            row = {
                'period_id': wfo_result.period.period_id,
                'training_start': wfo_result.period.training_start.isoformat(),
                'training_end': wfo_result.period.training_end.isoformat(),
                'testing_start': wfo_result.period.testing_start.isoformat(),
                'testing_end': wfo_result.period.testing_end.isoformat(),
                'best_backtest_id': wfo_result.best_backtest_id,
                'out_of_sample_return': wfo_result.out_of_sample_return,
                'out_of_sample_sharpe': wfo_result.out_of_sample_sharpe,
                'out_of_sample_max_drawdown': wfo_result.out_of_sample_max_drawdown,
                'stability_score': wfo_result.stability_score,
                'success': wfo_result.success,
                'error_message': wfo_result.error_message or ''
            }
            
            # Add training metrics
            for key, value in wfo_result.training_metrics.items():
                row[f'training_{key}'] = value
            
            # Add testing metrics
            for key, value in wfo_result.testing_metrics.items():
                row[f'testing_{key}'] = value
            
            rows.append(row)
        
        if _has_pandas:
            # Create DataFrame and save with pandas
            df = pd.DataFrame(rows)
            df.to_csv(output_path, index=False)
        else:
            # Save CSV manually without pandas
            import csv
            if rows:
                with open(output_path, 'w', newline='') as csvfile:
                    fieldnames = rows[0].keys()
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(rows)
        
        self.logger.info(f"üìä WFO report saved: {output_path}")
        return output_path

